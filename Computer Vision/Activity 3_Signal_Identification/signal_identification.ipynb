{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 3: Signal Identification\n",
    "### Module 2 - Computer Vision - Intelligent Robotics Implementation\n",
    "\n",
    "**Adrián Lozano González - A01661437** \n",
    "\n",
    "**Israel Macías Santana - A01027029**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries. *Deque* will be necessary to apply a kind of filter, that is, there will be a frame storage buffer to obtain the best match. This will be explained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions determine the obtaining of keypoints and descriptors, Hu moments, and finding the most common element in an array:\n",
    "\n",
    "*obtain_sift* uses a SIFT object to get the keypoints and descriptors of the provided image. This needs to be performed on a grayscale image. Alternatively, an ORB object could describe the same characteristics, however, it loses precision.\n",
    "\n",
    "*obtain_hu* uses the moments of the image to obtain the Hu moments. Previous to this operation, a grayscale conversion is needed and a global threshold needs to be applied. The most important quality of moments is that since intensity is relevant for M10, M00 and M01, the threshold needs to be perfectly calculated, thus keeping a threshold of 128. \n",
    "\n",
    "*most_common_element* creates a dictionary where each element is stored along with the number of times it has been repeated, from which it obtains the most frequent one. This will be necessary to find the correct sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_sift(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def obtain_hu(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)\n",
    "    moments = cv2.moments(thresh)\n",
    "    hu_moments = cv2.HuMoments(moments).flatten()\n",
    "    return hu_moments\n",
    "\n",
    "def most_common_element(buffer):\n",
    "    count_dict = {}\n",
    "    for item in buffer:\n",
    "        if item in count_dict:\n",
    "            count_dict[item] += 1\n",
    "        else:\n",
    "            count_dict[item] = 1\n",
    "    most_common_item = max(count_dict, key=count_dict.get)\n",
    "    return most_common_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, all the images to be compared are read. For each image, the descriptors, keypoints, and Hu moments are obtained using the previously described functions. Two lists are created. One will store each image descriptors so that we can compare later on. The second one stores each Hu moments, giving certan specified characteristics of each sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'signs'  \n",
    "image_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "\n",
    "descriptors_list = []\n",
    "hu_moments_list = []\n",
    "for image_path in image_paths:\n",
    "    img = cv2.imread(image_path)\n",
    "    _, descriptors = obtain_sift(img)\n",
    "    hu_moments = obtain_hu(img)\n",
    "    descriptors_list.append(descriptors)\n",
    "    hu_moments_list.append(hu_moments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SIFT object, Brute Force matcher, and video objects are initialized. Additionally, a buffer of 10 elements is created, which will be evaluated each time it is filled to obtain the most frequent occurrence and this will lead to smoothing selections and act as a \"mode\" filter, where the most repeated element in the buffer will be the definite value. This will be discussed on the main loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "sift = cv2.SIFT_create()\n",
    "bf = cv2.BFMatcher()\n",
    "\n",
    "\n",
    "best_match_buffer = deque(maxlen=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main video loop. Descriptors and keypoints of the current video frame, as well as the Hu moments, are calculated. Each descriptor (of each of the signs) is compared using a brute force knn match (where k=2 to obtain two matches), and both obtained matches are compared. If the first match is less than 70% of the second, we keep that match. This threshold will prevent matches that do not necessarily provide relevant descriptors and the lower it is, the mor restrictive it is, thus, obtaining more precision.\n",
    "\n",
    "Then, the magnitude of the vector that compares (subtracts) the Hu moments of the current frame with those of each sign is calculated. Here, we check if:\n",
    "\n",
    "1) There are more than 20 matches obtained.\n",
    "2) The distance between Hu moments is **minimal**.\n",
    "\n",
    "If both conditions are met, it means that this is most likely the sign, and thus the *best_index* becomes our current index.\n",
    "\n",
    "Finally, if the index is appropriate (not equal to -1, which means no suitable match or minimal Hu moments difference was found), it is added to the before mentioned buffer. Additionally, if the buffer is already full, the most repeated index is counted (as a filter), and so, the **correct sign** is obtained from the list of paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    keypoints_frame, descriptors_frame = sift.detectAndCompute(gray_frame, None)\n",
    "\n",
    "    if descriptors_frame is not None:\n",
    "        \n",
    "        _, thresh_frame = cv2.threshold(gray_frame, 128, 255, cv2.THRESH_BINARY)\n",
    "        moments_frame = cv2.moments(thresh_frame)\n",
    "        hu_moments_frame = cv2.HuMoments(moments_frame).flatten()\n",
    "\n",
    "        best_match_index = -1\n",
    "        best_match_score = float('inf')\n",
    "        best_match_good_matches = None\n",
    "\n",
    "        for i, descriptors in enumerate(descriptors_list):\n",
    "            matches = bf.knnMatch(descriptors, descriptors_frame, k=2)\n",
    "            good_matches = []\n",
    "            for m, n in matches:\n",
    "                if m.distance < 0.7 * n.distance:  \n",
    "                    good_matches.append(m)\n",
    "\n",
    "        \n",
    "            hu_distance = np.linalg.norm(hu_moments_list[i] - hu_moments_frame)\n",
    "\n",
    "           \n",
    "            if len(good_matches) > 20 and hu_distance < best_match_score:  \n",
    "                best_match_index = i\n",
    "                best_match_score = hu_distance\n",
    "                best_match_good_matches = good_matches\n",
    "\n",
    "        if best_match_index != -1:\n",
    "            best_match_buffer.append(best_match_index)\n",
    "\n",
    "            if len(best_match_buffer) == best_match_buffer.maxlen:\n",
    "                most_common_match = most_common_element(best_match_buffer)\n",
    "                print(f\"Most common match: {image_paths[most_common_match]}\")\n",
    "\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
